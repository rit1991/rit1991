{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook --no-browser\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecommerce Churn Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the assignment is to build a model that predicts whether a person purchases an item after it has been added to the cart or not. Being a classification problem, you are expected to use your understanding of all the three models covered till now. You must select the most robust model and provide a solution that predicts the churn in the most suitable manner. \n",
    "\n",
    "For this assignment, you are provided the data associated with an e-commerce company for the month of October 2019. Your task is to first analyse the data, and then perform multiple steps towards the model building process.\n",
    "\n",
    "The broad tasks are:\n",
    "- Data Exploration\n",
    "- Feature Engineering\n",
    "- Model Selection\n",
    "- Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset stores the information of a customer session on the e-commerce platform. It records the activity and the associated parameters with it.\n",
    "\n",
    "- **event_time**: Date and time when user accesses the platform\n",
    "- **event_type**: Action performed by the customer\n",
    "            - View\n",
    "            - Cart\n",
    "            - Purchase\n",
    "            - Remove from cart\n",
    "- **product_id**: Unique number to identify the product in the event\n",
    "- **category_id**: Unique number to identify the category of the product\n",
    "- **category_code**: Stores primary and secondary categories of the product\n",
    "- **brand**: Brand associated with the product\n",
    "- **price**: Price of the product\n",
    "- **user_id**: Unique ID for a customer\n",
    "- **user_session**: Session ID for a user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided is 5 GBs in size. Therefore, it is expected that you increase the driver memory to a greater number. You can refer to notebook 1 for the steps involved here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark environment\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-71-243.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd7143ac110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialising the session with 14 GB driver memory\n",
    "MAX_MEMORY = \"14G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"demo\") \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14G'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().get('spark.driver.memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the clean data\n",
    "\n",
    "df = spark.read.parquet('cleaned_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model Selection\n",
    "3 models for classification:\t\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional steps for Logistic regression - Feature selection, Correlation, etc.\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator,StringIndexer,VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- event_day: integer (nullable = true)\n",
      " |-- sub_categ: string (nullable = true)\n",
      " |-- user_session_activity: long (nullable = true)\n",
      " |-- user_product_count: long (nullable = true)\n",
      " |-- sub_categ_user_count: long (nullable = true)\n",
      " |-- prod_avg_spend: double (nullable = true)\n",
      " |-- user_sess_count: long (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if only the required columns are present to build the model\n",
    "# If not, drop the redundant columns\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorising the attributes into its type - Continuous and Categorical\n",
    "\n",
    "categ_features=['brand','sub_categ', 'event_day']\n",
    "cont_features=['price', 'user_product_count', 'sub_categ_user_count', 'prod_avg_spend',\\\n",
    "                     'user_sess_count', 'user_session_activity']\n",
    "output_label = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation for categorical features\n",
    "\n",
    "for categoricalCol in categ_features:\n",
    "    stringIndexer=StringIndexer(inputCol=categoricalCol,outputCol=categoricalCol+'_ind').setHandleInvalid(\"keep\")\n",
    "    encoder=OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()],outputCols=\\\n",
    "                                   [categoricalCol+\"_enc\"])\n",
    "    stages+=[stringIndexer,encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector assembler to combine all the features\n",
    "\n",
    "assemblerInputs=[col+\"_enc\" for col in categ_features]+cont_features\n",
    "assembler=VectorAssembler(inputCols=assemblerInputs,outputCol=\"features\")\n",
    "stages+=[assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for the tasks\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline=Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the dataframe df\n",
    "\n",
    "df_transform = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- event_day: integer (nullable = true)\n",
      " |-- sub_categ: string (nullable = true)\n",
      " |-- user_session_activity: long (nullable = true)\n",
      " |-- user_product_count: long (nullable = true)\n",
      " |-- sub_categ_user_count: long (nullable = true)\n",
      " |-- prod_avg_spend: double (nullable = true)\n",
      " |-- user_sess_count: long (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- brand_ind: double (nullable = false)\n",
      " |-- brand_enc: vector (nullable = true)\n",
      " |-- sub_categ_ind: double (nullable = false)\n",
      " |-- sub_categ_enc: vector (nullable = true)\n",
      " |-- event_day_ind: double (nullable = false)\n",
      " |-- event_day_enc: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema of the transformed df\n",
    "\n",
    "df_transform.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+----------+---------------------+------------------+--------------------+------------------+---------------+-----+---------+--------------+-------------+--------------+-------------+-------------+--------------------------------------------------------------------------------------------+\n",
      "|brand   |price  |event_day|sub_categ |user_session_activity|user_product_count|sub_categ_user_count|prod_avg_spend    |user_sess_count|label|brand_ind|brand_enc     |sub_categ_ind|sub_categ_enc |event_day_ind|event_day_enc|features                                                                                    |\n",
      "+--------+-------+---------+----------+---------------------+------------------+--------------------+------------------+---------------+-----+---------+--------------+-------------+--------------+-------------+-------------+--------------------------------------------------------------------------------------------+\n",
      "|samsung |283.62 |3        |smartphone|2                    |2                 |3                   |270.4633333333333 |2              |0    |0.0      |(20,[0],[1.0])|0.0          |(38,[0],[1.0])|1.0          |(7,[1],[1.0])|(71,[0,20,59,65,66,67,68,69,70],[1.0,1.0,1.0,283.62,2.0,3.0,270.4633333333333,2.0,2.0])     |\n",
      "|xiaomi  |138.71 |4        |smartphone|2                    |2                 |3                   |213.38666666666668|2              |0    |3.0      |(20,[3],[1.0])|0.0          |(38,[0],[1.0])|2.0          |(7,[2],[1.0])|(71,[3,20,60,65,66,67,68,69,70],[1.0,1.0,1.0,138.71,2.0,3.0,213.38666666666668,2.0,2.0])    |\n",
      "|apple   |581.83 |4        |smartphone|9                    |9                 |14                  |643.4971428571429 |4              |0    |1.0      |(20,[1],[1.0])|0.0          |(38,[0],[1.0])|2.0          |(7,[2],[1.0])|(71,[1,20,60,65,66,67,68,69,70],[1.0,1.0,1.0,581.83,9.0,14.0,643.4971428571429,4.0,9.0])    |\n",
      "|apple   |460.54 |6        |smartphone|3                    |5                 |6                   |408.0316666666667 |3              |0    |1.0      |(20,[1],[1.0])|0.0          |(38,[0],[1.0])|0.0          |(7,[0],[1.0])|(71,[1,20,58,65,66,67,68,69,70],[1.0,1.0,1.0,460.54,5.0,6.0,408.0316666666667,3.0,3.0])     |\n",
      "|samsung |369.37 |7        |video     |6                    |6                 |6                   |369.36999999999995|3              |1    |0.0      |(20,[0],[1.0])|3.0          |(38,[3],[1.0])|5.0          |(7,[5],[1.0])|(71,[0,23,63,65,66,67,68,69,70],[1.0,1.0,1.0,369.37,6.0,6.0,369.36999999999995,3.0,6.0])    |\n",
      "|samsung |369.37 |7        |video     |6                    |6                 |6                   |369.36999999999995|3              |1    |0.0      |(20,[0],[1.0])|3.0          |(38,[3],[1.0])|5.0          |(7,[5],[1.0])|(71,[0,23,63,65,66,67,68,69,70],[1.0,1.0,1.0,369.37,6.0,6.0,369.36999999999995,3.0,6.0])    |\n",
      "|elenberg|50.91  |1        |kitchen   |5                    |4                 |5                   |68.974            |3              |1    |8.0      |(20,[8],[1.0])|1.0          |(38,[1],[1.0])|3.0          |(7,[3],[1.0])|(71,[8,21,61,65,66,67,68,69,70],[1.0,1.0,1.0,50.91,4.0,5.0,68.974,3.0,5.0])                 |\n",
      "|apple   |746.09 |1        |smartphone|5                    |4                 |5                   |797.0260000000001 |1              |1    |1.0      |(20,[1],[1.0])|0.0          |(38,[0],[1.0])|3.0          |(7,[3],[1.0])|(71,[1,20,61,65,66,67,68,69,70],[1.0,1.0,1.0,746.09,4.0,5.0,797.0260000000001,1.0,5.0])     |\n",
      "|apple   |1634.28|3        |smartphone|14                   |10                |24                  |1816.1112499999997|3              |0    |1.0      |(20,[1],[1.0])|0.0          |(38,[0],[1.0])|1.0          |(7,[1],[1.0])|(71,[1,20,59,65,66,67,68,69,70],[1.0,1.0,1.0,1634.28,10.0,24.0,1816.1112499999997,3.0,14.0])|\n",
      "|apple   |1634.28|3        |smartphone|14                   |10                |24                  |1816.1112499999997|3              |0    |1.0      |(20,[1],[1.0])|0.0          |(38,[0],[1.0])|1.0          |(7,[1],[1.0])|(71,[1,20,59,65,66,67,68,69,70],[1.0,1.0,1.0,1634.28,10.0,24.0,1816.1112499999997,3.0,14.0])|\n",
      "|apple   |1349.44|5        |smartphone|8                    |4                 |8                   |1468.8200000000002|2              |0    |1.0      |(20,[1],[1.0])|0.0          |(38,[0],[1.0])|4.0          |(7,[4],[1.0])|(71,[1,20,62,65,66,67,68,69,70],[1.0,1.0,1.0,1349.44,4.0,8.0,1468.8200000000002,2.0,8.0])   |\n",
      "|apple   |1349.44|5        |smartphone|8                    |4                 |8                   |1468.8200000000002|2              |0    |1.0      |(20,[1],[1.0])|0.0          |(38,[0],[1.0])|4.0          |(7,[4],[1.0])|(71,[1,20,62,65,66,67,68,69,70],[1.0,1.0,1.0,1349.44,4.0,8.0,1468.8200000000002,2.0,8.0])   |\n",
      "|apple   |1588.2 |5        |smartphone|8                    |4                 |8                   |1468.8200000000002|2              |1    |1.0      |(20,[1],[1.0])|0.0          |(38,[0],[1.0])|4.0          |(7,[4],[1.0])|(71,[1,20,62,65,66,67,68,69,70],[1.0,1.0,1.0,1588.2,4.0,8.0,1468.8200000000002,2.0,8.0])    |\n",
      "|samsung |197.15 |1        |smartphone|8                    |4                 |58                  |203.15948275862078|6              |0    |0.0      |(20,[0],[1.0])|0.0          |(38,[0],[1.0])|3.0          |(7,[3],[1.0])|(71,[0,20,61,65,66,67,68,69,70],[1.0,1.0,1.0,197.15,4.0,58.0,203.15948275862078,6.0,8.0])   |\n",
      "|samsung |172.23 |5        |smartphone|4                    |4                 |58                  |203.15948275862078|6              |1    |0.0      |(20,[0],[1.0])|0.0          |(38,[0],[1.0])|4.0          |(7,[4],[1.0])|(71,[0,20,62,65,66,67,68,69,70],[1.0,1.0,1.0,172.23,4.0,58.0,203.15948275862078,6.0,4.0])   |\n",
      "|samsung |233.66 |2        |smartphone|15                   |3                 |58                  |203.15948275862078|6              |1    |0.0      |(20,[0],[1.0])|0.0          |(38,[0],[1.0])|6.0          |(7,[6],[1.0])|(71,[0,20,64,65,66,67,68,69,70],[1.0,1.0,1.0,233.66,3.0,58.0,203.15948275862078,6.0,15.0])  |\n",
      "|samsung |229.09 |6        |smartphone|16                   |4                 |58                  |203.15948275862078|6              |1    |0.0      |(20,[0],[1.0])|0.0          |(38,[0],[1.0])|0.0          |(7,[0],[1.0])|(71,[0,20,58,65,66,67,68,69,70],[1.0,1.0,1.0,229.09,4.0,58.0,203.15948275862078,6.0,16.0])  |\n",
      "|samsung |246.96 |3        |smartphone|17                   |5                 |27                  |241.41111111111118|3              |1    |0.0      |(20,[0],[1.0])|0.0          |(38,[0],[1.0])|1.0          |(7,[1],[1.0])|(71,[0,20,59,65,66,67,68,69,70],[1.0,1.0,1.0,246.96,5.0,27.0,241.41111111111118,3.0,17.0])  |\n",
      "|samsung |224.38 |3        |smartphone|17                   |6                 |27                  |241.41111111111118|3              |1    |0.0      |(20,[0],[1.0])|0.0          |(38,[0],[1.0])|1.0          |(7,[1],[1.0])|(71,[0,20,59,65,66,67,68,69,70],[1.0,1.0,1.0,224.38,6.0,27.0,241.41111111111118,3.0,17.0])  |\n",
      "|apple   |975.56 |1        |smartphone|7                    |11                |14                  |1022.7078571428568|4              |1    |1.0      |(20,[1],[1.0])|0.0          |(38,[0],[1.0])|3.0          |(7,[3],[1.0])|(71,[1,20,61,65,66,67,68,69,70],[1.0,1.0,1.0,975.56,11.0,14.0,1022.7078571428568,4.0,7.0])  |\n",
      "+--------+-------+---------+----------+---------------------+------------------+--------------------+------------------+---------------+-----+---------+--------------+-------------+--------------+-------------+-------------+--------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the elements of the transformed df - Top 20 rows\n",
    "\n",
    "df_transform.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the transformed df in S3 bucket to prevent repetition of steps again\n",
    "\n",
    "df_transform.coalesce(1).write.option(\"header\", \"true\").parquet(\"LR_Transform_Output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "scaler = MinMaxScaler().setInputCol(\"features\").setOutputCol(\"scaled_features\")\n",
    "df_logistic = scaler.fit(df_transform).transform(df_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(71, {0: 1.0, 20: 1.0, 59: 1.0, 65: 283.62, 66: 2.0, 67: 3.0, 68: 270.4633, 69: 2.0, 70: 2.0}), scaled_features=DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1098, 0.0013, 0.0006, 0.1047, 0.0005, 0.0031]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaled features\n",
    "df_logistic.select(\"features\",\"scaled_features\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test (Remember you are expected to compare the model later)\n",
    "\n",
    "traindata, testdata = df_logistic.randomSplit([0.7,0.3], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549095"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows in train and test data\n",
    "\n",
    "traindata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235266"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='scaled_features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model on transformed df\n",
    "\n",
    "model = lr.fit(traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Steps:\n",
    "- Fit on test data\n",
    "- Performance analysis\n",
    "    - Appropriate Metric with reasoning\n",
    "    - Threshold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.016364044126548516,0.024748699218964852,-0.28785411121093746,-0.5358607494044827,-0.12917960496283934,0.011084880414660566,-0.15600180236201014,-0.09730783673971405,-0.22149768095428427,-0.24177574521820927,-0.25333938435742426,-0.19464539597003264,-0.1661891541166122,-0.24003551972653012,-0.3894712611289704,-0.13530529335577177,-0.31371923167152166,-0.3802748846488473,-0.4285691598795496,0.2820335114163284,0.007787971362826653,-0.3031394080317867,-0.40911510160185927,-0.2760195166770672,-0.22582200766893348,-0.3757056101442078,-0.2518806030280961,-0.39938026186328257,-0.1910907039604234,-0.479464854549623,-0.28593963054690597,-0.33670391225922214,-0.051894608494017135,-0.5367769536868628,-0.2882989288266407,-0.17619677321155902,-0.5437305869579929,-0.4027020140216677,-0.6813934772732959,-0.8823411058744571,-0.5454344061012408,-0.39700131305355363,-0.5762834944594801,-0.9708166145313164,0.11174865396798954,-0.5629084306812733,-0.6307816949116098,-1.1923134522613483,-0.5402321881859231,0.24462557600536508,-0.7527086286158978,-1.0031286075515446,0.13788915102662797,-1.101650742250047,1.6450014998326448,-0.8251282211895073,7.602329190333531,-9.542041341240964,-0.18360399303159916,-0.13030437166410302,-0.03577031997559343,-0.13549390400625047,-0.05774304966982288,-0.16552715708261687,-0.09536527761537428,-0.9492842642368239,60.87039985803459,-7.3396878422204175,0.1402904914607828,-1.71522426660582,-6.3134995716674265]\n",
      "Intercept: 0.39020371720436814\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = model.transform(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = model.transform(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       1.0|\n",
      "|    0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_test.select('label', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.7082473906942639\n",
      "Test Area Under ROC 0.708299445894665\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions_train))\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMeasure on Training data [0.44304921672628583, 0.7666291946091923]\n",
      "fMeasure on Test data [0.4420633551799471, 0.7657130786408353]\n"
     ]
    }
   ],
   "source": [
    "result_train = model.evaluate(traindata)\n",
    "result_test = model.evaluate(testdata)\n",
    "\n",
    "print('fMeasure on Training data', result_train.fMeasureByLabel())\n",
    "print('fMeasure on Test data', result_test.fMeasureByLabel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = predictions_train.select('label','probability','prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, when\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "element_extrac=udf(lambda v:float(v[1]),FloatType())\n",
    "\n",
    "final_result = conf_matrix.withColumn('label_p',when(element_extrac(conf_matrix['probability']) >= 0.5, 1).otherwise(0))\n",
    "TP = final_result.filter(\"label==1 AND label_p==1\").count()\n",
    "FP = final_result.filter(\"label==0 AND label_p==1\").count()\n",
    "FN = final_result.filter(\"label==1 AND label_p==0\").count()\n",
    "TN = final_result.filter(\"label==0 AND label_p==0\").count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6710805962538359"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6843002468224493"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = (TP)/(TP+FP)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8714776733254994"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = (TP)/(TP+FN)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666291946091923"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fscore = (2*precision*recall)/(precision + recall)\n",
    "fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updated model, if any\n",
    "Repeat the steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
